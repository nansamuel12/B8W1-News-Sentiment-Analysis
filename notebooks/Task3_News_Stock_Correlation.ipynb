{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Correlation between News Sentiment and Stock Movement\n",
    "\n",
    "## Objective\n",
    "Analyze the correlation between news sentiment and stock price movements:\n",
    "1. **Date Alignment**: Align news and stock data by dates\n",
    "2. **Sentiment Analysis**: Quantify news headline sentiment\n",
    "3. **Correlation Analysis**: Test correlation between sentiment and stock returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load news data from raw_analyst_ratings.csv\n",
    "news_df = pd.read_csv('../data/raw_analyst_ratings.csv')\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], utc=True)\n",
    "news_df['date'] = news_df['date'].dt.tz_localize(None)  # Remove timezone for easier merging\n",
    "\n",
    "print(f\"News data: {len(news_df)} records\")\n",
    "print(f\"Date range: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "print(f\"Unique stocks: {news_df['stock'].nunique()}\")\n",
    "print(f\"Unique publishers: {news_df['publisher'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stock data and map stock symbols\n",
    "tickers = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "stock_data = {}\n",
    "\n",
    "# Create mapping for stock symbols in news data\n",
    "stock_mapping = {\n",
    "    'AAPL': 'AAPL',\n",
    "    'AMZN': 'AMZN', \n",
    "    'GOOG': 'GOOG',\n",
    "    'GOOGL': 'GOOG',  # Google has multiple tickers\n",
    "    'META': 'META',\n",
    "    'FB': 'META',     # Facebook renamed to Meta\n",
    "    'MSFT': 'MSFT',\n",
    "    'NVDA': 'NVDA'\n",
    "}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = pd.read_csv(f'../data/Data/{ticker}.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    df['Daily_Return'] = df['Close'].pct_change()\n",
    "    stock_data[ticker] = df\n",
    "    print(f\"{ticker}: {len(df)} records from {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "\n",
    "print(f\"\\nStock data loaded for {len(tickers)} tickers\")\n",
    "stock_data['AAPL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Sentiment Analysis on News Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzers\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_textblob_sentiment(text):\n",
    "    \"\"\"Get sentiment using TextBlob\"\"\"\n",
    "    try:\n",
    "        return TextBlob(str(text)).sentiment.polarity\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    \"\"\"Get sentiment using VADER\"\"\"\n",
    "    try:\n",
    "        return vader.polarity_scores(str(text))['compound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Calculate sentiment scores\n",
    "print(\"Calculating sentiment scores...\")\n",
    "news_df['sentiment_textblob'] = news_df['headline'].apply(get_textblob_sentiment)\n",
    "news_df['sentiment_vader'] = news_df['headline'].apply(get_vader_sentiment)\n",
    "\n",
    "# Average sentiment score\n",
    "news_df['sentiment_avg'] = (news_df['sentiment_textblob'] + news_df['sentiment_vader']) / 2\n",
    "\n",
    "# Sentiment category\n",
    "news_df['sentiment_category'] = pd.cut(\n",
    "    news_df['sentiment_avg'],\n",
    "    bins=[-1, -0.05, 0.05, 1],\n",
    "    labels=['Negative', 'Neutral', 'Positive']\n",
    ")\n",
    "\n",
    "print(\"Sentiment analysis complete\")\n",
    "news_df[['headline', 'sentiment_textblob', 'sentiment_vader', 'sentiment_avg', 'sentiment_category']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(news_df['sentiment_avg'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_title('Distribution of Sentiment Scores', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Category counts\n",
    "sentiment_counts = news_df['sentiment_category'].value_counts()\n",
    "axes[1].bar(sentiment_counts.index, sentiment_counts.values, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Sentiment Category Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSentiment Statistics:\")\n",
    "print(news_df['sentiment_avg'].describe())\n",
    "print(f\"\\nSentiment Categories:\")\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Aggregate Daily Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sentiment by date for each stock\n",
    "# We'll create both overall daily sentiment and stock-specific sentiment\n",
    "\n",
    "# Overall daily sentiment (all news)\n",
    "daily_sentiment_all = news_df.groupby(news_df['date'].dt.date).agg({\n",
    "    'sentiment_avg': ['mean', 'std', 'count'],\n",
    "    'sentiment_textblob': 'mean',\n",
    "    'sentiment_vader': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "daily_sentiment_all.columns = ['date', 'sentiment_mean', 'sentiment_std', 'news_count', 'textblob_mean', 'vader_mean']\n",
    "daily_sentiment_all['date'] = pd.to_datetime(daily_sentiment_all['date'])\n",
    "\n",
    "print(f\"Overall daily sentiment aggregated: {len(daily_sentiment_all)} days\")\n",
    "\n",
    "# Stock-specific daily sentiment\n",
    "stock_sentiment = {}\n",
    "for ticker in tickers:\n",
    "    # Filter news for this specific stock\n",
    "    stock_news = news_df[news_df['stock'] == ticker].copy()\n",
    "    \n",
    "    if len(stock_news) > 0:\n",
    "        daily = stock_news.groupby(stock_news['date'].dt.date).agg({\n",
    "            'sentiment_avg': ['mean', 'count']\n",
    "        }).reset_index()\n",
    "        daily.columns = ['date', 'sentiment_mean', 'news_count']\n",
    "        daily['date'] = pd.to_datetime(daily['date'])\n",
    "        stock_sentiment[ticker] = daily\n",
    "        print(f\"{ticker}: {len(stock_news)} news articles, {len(daily)} unique days\")\n",
    "    else:\n",
    "        print(f\"{ticker}: No specific news found\")\n",
    "\n",
    "print(\"\\nUsing overall daily sentiment for correlation analysis\")\n",
    "daily_sentiment_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily sentiment over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Sentiment mean\n",
    "axes[0].plot(daily_sentiment['date'], daily_sentiment['sentiment_mean'], linewidth=1.5)\n",
    "axes[0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].fill_between(daily_sentiment['date'], daily_sentiment['sentiment_mean'], 0, alpha=0.3)\n",
    "axes[0].set_title('Daily Average Sentiment Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Sentiment Score')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# News count\n",
    "axes[1].bar(daily_sentiment['date'], daily_sentiment['news_count'], alpha=0.7, width=1)\n",
    "axes[1].set_title('Daily News Article Count', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Number of Articles')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Merge Sentiment with Stock Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentiment with stock data for each ticker\n",
    "merged_data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    stock_df = stock_data[ticker].copy()\n",
    "    stock_df['date'] = pd.to_datetime(stock_df['Date'].dt.date)\n",
    "    \n",
    "    # Try stock-specific sentiment first, fall back to overall sentiment\n",
    "    if ticker in stock_sentiment and len(stock_sentiment[ticker]) > 10:\n",
    "        sentiment_to_use = stock_sentiment[ticker]\n",
    "        sentiment_type = \"stock-specific\"\n",
    "    else:\n",
    "        sentiment_to_use = daily_sentiment_all\n",
    "        sentiment_type = \"overall market\"\n",
    "    \n",
    "    # Merge with sentiment\n",
    "    merged = pd.merge(\n",
    "        stock_df[['date', 'Close', 'Daily_Return', 'Volume']],\n",
    "        sentiment_to_use[['date', 'sentiment_mean', 'news_count']],\n",
    "        on='date',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    merged = merged.dropna()\n",
    "    merged_data[ticker] = merged\n",
    "    print(f\"{ticker}: {len(merged)} days with both sentiment ({sentiment_type}) and stock data\")\n",
    "\n",
    "print(\"\\nSample merged data for AAPL:\")\n",
    "merged_data['AAPL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations for each stock\n",
    "correlation_results = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = merged_data[ticker]\n",
    "    \n",
    "    # Pearson correlation\n",
    "    corr_pearson, p_value_pearson = stats.pearsonr(df['sentiment_mean'], df['Daily_Return'])\n",
    "    \n",
    "    # Spearman correlation\n",
    "    corr_spearman, p_value_spearman = stats.spearmanr(df['sentiment_mean'], df['Daily_Return'])\n",
    "    \n",
    "    correlation_results[ticker] = {\n",
    "        'Pearson_Corr': corr_pearson,\n",
    "        'Pearson_PValue': p_value_pearson,\n",
    "        'Spearman_Corr': corr_spearman,\n",
    "        'Spearman_PValue': p_value_spearman,\n",
    "        'Sample_Size': len(df)\n",
    "    }\n",
    "\n",
    "corr_df = pd.DataFrame(correlation_results).T\n",
    "print(\"\\nCorrelation Analysis Results:\")\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(tickers))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, corr_df['Pearson_Corr'], width, label='Pearson', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, corr_df['Spearman_Corr'], width, label='Spearman', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Stock Ticker')\n",
    "ax.set_ylabel('Correlation Coefficient')\n",
    "ax.set_title('Correlation between News Sentiment and Stock Returns', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers)\n",
    "ax.legend()\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Scatter Plots - Sentiment vs Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for each stock\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    df = merged_data[ticker]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(df['sentiment_mean'], df['Daily_Return'] * 100, alpha=0.5, s=20)\n",
    "    \n",
    "    # Regression line\n",
    "    z = np.polyfit(df['sentiment_mean'], df['Daily_Return'] * 100, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(df['sentiment_mean'], p(df['sentiment_mean']), \"r--\", linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Correlation info\n",
    "    corr = correlation_results[ticker]['Pearson_Corr']\n",
    "    pval = correlation_results[ticker]['Pearson_PValue']\n",
    "    \n",
    "    ax.set_title(f'{ticker} - Sentiment vs Returns\\nCorr={corr:.3f}, p={pval:.4f}', fontweight='bold')\n",
    "    ax.set_xlabel('Daily Sentiment Score')\n",
    "    ax.set_ylabel('Daily Return (%)')\n",
    "    ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Lagged Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test lagged correlations (sentiment today vs returns tomorrow)\n",
    "lagged_correlations = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = merged_data[ticker].copy()\n",
    "    \n",
    "    lags = {}\n",
    "    for lag in range(0, 6):  # 0 to 5 days lag\n",
    "        df[f'Return_lag{lag}'] = df['Daily_Return'].shift(-lag)\n",
    "        corr, pval = stats.pearsonr(df['sentiment_mean'].dropna(), df[f'Return_lag{lag}'].dropna())\n",
    "        lags[f'Lag_{lag}'] = corr\n",
    "    \n",
    "    lagged_correlations[ticker] = lags\n",
    "\n",
    "lagged_df = pd.DataFrame(lagged_correlations).T\n",
    "print(\"\\nLagged Correlation Analysis:\")\n",
    "print(\"(Sentiment today vs Returns in N days)\")\n",
    "lagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lagged correlations\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for ticker in tickers:\n",
    "    plt.plot(range(6), lagged_df.loc[ticker], marker='o', label=ticker, linewidth=2)\n",
    "\n",
    "plt.title('Lagged Correlation: Sentiment vs Future Returns', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Lag (Days)')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.legend()\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. SENTIMENT STATISTICS:\")\n",
    "print(f\"   - Total news articles analyzed: {len(news_df)}\")\n",
    "print(f\"   - Average sentiment score: {news_df['sentiment_avg'].mean():.4f}\")\n",
    "print(f\"   - Sentiment std deviation: {news_df['sentiment_avg'].std():.4f}\")\n",
    "\n",
    "print(\"\\n2. CORRELATION STRENGTH (Pearson):\")\n",
    "for ticker in tickers:\n",
    "    corr = correlation_results[ticker]['Pearson_Corr']\n",
    "    pval = correlation_results[ticker]['Pearson_PValue']\n",
    "    sig = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"ns\"\n",
    "    print(f\"   {ticker}: {corr:+.4f} ({sig})\")\n",
    "\n",
    "print(\"\\n3. KEY FINDINGS:\")\n",
    "strongest = corr_df['Pearson_Corr'].abs().idxmax()\n",
    "print(f\"   - Strongest correlation: {strongest} ({corr_df.loc[strongest, 'Pearson_Corr']:.4f})\")\n",
    "print(f\"   - Average correlation: {corr_df['Pearson_Corr'].mean():.4f}\")\n",
    "print(f\"   - Significant correlations (p<0.05): {(corr_df['Pearson_PValue'] < 0.05).sum()}/{len(tickers)}\")\n",
    "\n",
    "print(\"\\nNote: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
